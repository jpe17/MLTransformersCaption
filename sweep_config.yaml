program: 08_train_explained_wandb.py
method: bayes
metric:
  goal: minimize
  name: final_train_loss
parameters:
  # Training parameters
  num_epochs:
    value: 2
  
  # Optimizer parameters
  optimizer:
    values: ["adamw", "adam"]
  
  learning_rate:
    distribution: log_uniform
    min: 1e-6
    max: 5e-5  # More conservative max learning rate
  
  weight_decay:
    distribution: log_uniform
    min: 0.001
    max: 0.01  # More conservative weight decay
  
  beta1:
    distribution: uniform
    min: 0.85
    max: 0.95
  
  beta2:
    distribution: uniform
    min: 0.99
    max: 0.999
  
  momentum:
    distribution: uniform
    min: 0.85
    max: 0.95
  
  # Scheduler parameters
  scheduler:
    values: ["cosine", "linear", "constant"]
  
  # Gradient clipping
  grad_clip_norm:
    distribution: uniform
    min: 0.1
    max: 1.0  # More aggressive gradient clipping
  
  # Generation parameters
  top_k:
    distribution: int_uniform
    min: 20
    max: 100
  
  temperature:
    distribution: uniform
    min: 0.7
    max: 1.3

# Sweep configuration
project: image-captioning-sweep
name: cross-attention-model-sweep
description: "Hyperparameter sweep for cross-attention image captioning model"
count: 20  # Number of runs in the sweep 